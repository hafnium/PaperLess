\section{System Overview}
\label{sec:overview}

In this section we present the system-level implementation of our
approach. We describe the various components to our system and
describe the interactions between them. This is not meant to be a
formal specification, but rather a more informal description of our
specific implementation. For a more formal description of our system
please see the supplemental material.

We now describe our system in two parts. The first is the receipt
processing group, which is responsible for receiving, processing, and
saving receipts from customers. The second part of this section
describes the backend data warehousing group. We use this group to
store the receipt data from the processing group for future access and
analysis.

\subsection{Receipt Processing}
\label{sec:overview.processing}

A crucial property of our entire system is that it scale well with
the demands of the customers. As such, our design of the receipt
processing pool reflects this goal. The vision of our system is that
it will able to efficiently and reliably serve many millions of
requests with little or no degradation of service. We set out to
accomplish this by attempting to minimize possible downtime and/or
degradation of service. Our approach is twofold. First, we have
designed our system to be redundant. If any particular subsystem is
faulty for any reason, then our entire system is able to adapt and use
other resources to ``fill in the gaps.'' Unfortunately redundancy is
only half the battle. Even a highly redundant system can fail to
satisfy the needs of a growing number of requests. Therefore, as our
second goal, we wish to create a system that is also highly scalable.

Figure~\ref{fig:overview} is an overview of our system. The entry
point to the entire processing portion is the gateway. All receipt
requests arrive at the gateway. From there, our ProcessDistributor
attempts to schedule the receipt to be processed by a particular
ReceiptProcessingServer. Each receipt is assigned its own thread that
will be responsible for communicating with a particular processing
server. The threads themselves are designed to be lightweight and
completely independent. This allows the gateway to handle a large
number of receipts at once. Although this puts a critical confidence
in the gateway servers, we are able to provide redundancy and
scalability by using a simple scheme that uses DNS to choose an
alternative gateway if one goes down or to simply add a new one to the
pool and add a DNS entry if demand is higher.

One important issue for the gateway is its ability to
load-balance. This is accomplished by keeping a list of recently
scheduled jobs for each server. If a server does not respond to a
request, it is put on a blacklist and another server is used
instead. Periodically the distributor will check its blacklist to see
if any of those servers have come back online. New servers can be
easily added and removed by simply updating these lists allowing us to
easily scale our entire system as needed.

The servers themselves can be run on effectively any hardware with an
Internet connection. The server process simply polls its listening
socket for any connections from the gateway, and upon receiving a
request, processes that request and returns the result. Specifically,
when a processing server receives a request to process a receipt, it
first loads the receipt from a redundant distributed file system,
performs optical character recognition (OCR), parses the result into a
set of lines, and saves the information to our data warehouse.

\subsection{Database Overview}
\label{sec:overview.db}

As previously stated a primary goal of our system is that it is scallable.  To accomodate this requirement on the backend, we elected to run our data warehouse on a MySql Cluster.  The two most appealing aspects of MySql Cluster for our project are that it allows for multiple management nodes and that storage nodes can be added and removed on the fly.  It is essential that in the absence of catastrophic failure our system remains functional.  The system must be able to recover gracefully from errors such as power failures, disk failures, connection failures, etc.  MySql accomodates this necessity by automatically replicating data over storage nodes when a storage node fails to satisfy our desired redundancy.

In accordance with our requirement that there be no single point of failure in our system we decided that a distributed architecture was essential.  We have designed the system such that the data warehouse consists of multiple geographically separated locations which together represent the entire database.

Another crucial aspect of the MySql cluster is that it load balances automatically.  This was a major benefeit for our project because while we wanted to develop a system that was customized to our needs, we didn't want to re-invent the wheel.  We believe that any issues regarding load-balancing in MySql Cluster are low-risk.  If load balancing becomes an issue, it can be resolved easily by adding more management and storage nodes.  It may not be an elegant solution, but commodity hardware is cheap and the benefeit of developing a custom load balancer to suit our needs does not outway the effort and time required to do so.
